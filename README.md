# LMR-CL : Learning Modality-Fused Representations with Contrastive Loss for Multimodal Emotion Recognition 

## â—ï¸ Project Summary

---

1. **ì§„í–‰ ê¸°ê°„:** 2023.02 ~ 2023.07
2. **ì—­í• :** í”„ë¡œì íŠ¸ ë¦¬ë”, í…ìŠ¤íŠ¸ íŠ¹ì§• ì¶”ì¶œ, Contrative Loss ìˆ˜ì • ë° ëª¨ë¸ë§
3. **ê¸°ìˆ  ìŠ¤íƒ:**  **`Pytorch`**, **`HuggingFace`**
4. **ê²°ê³¼ ë° ì„±ê³¼:** 
    - íŠ¹í—ˆ ì¶œì›: ë©€í‹°ëª¨ë‹¬ ë°ì´í„°ì˜ ìœµí•©ì„ ê¸°ë°˜ìœ¼ë¡œ ê°ì •ì„ ì¸ì‹í•˜ê¸° ìœ„í•œ ì¥ì¹˜ ë° ë°©ë²•
    *(ì¶œì› ë²ˆí˜¸: 10-2023-0160075)*
    - ETRI íœ´ë¨¼ì´í•´ ì¸ê³µì§€ëŠ¥ ë…¼ë¬¸ê²½ì§„ëŒ€íšŒ ê³¼í•™ê¸°ìˆ ì •ë³´í†µì‹ ë¶€ì¥ê´€ìƒ ìˆ˜ìƒ [[ğŸ…]](https://drive.google.com/file/d/1nsln5z21XBjtUogHpP5YfbaCQmJUwiBA/view)
    - ë…¼ë¬¸ ê²Œì¬ [[ğŸ“„]](https://drive.google.com/file/d/1rNUtc2rlhUsqbbH1JOBMWbzLYgsePHfj/view)
5. **ì£¼ìš” ë‚´ìš©:** ë‘ ë°œí™”ìì˜ ììœ  ë°œí™” ê³¼ì •ì—ì„œì˜ í…ìŠ¤íŠ¸, ìŒì„±, ìƒì²´ì‹ í˜¸ë¡œ êµ¬ì„±ëœ ë©€í‹°ëª¨ë‹¬ ê°ì • ë°ì´í„° ì…‹ì„ ì´ìš©í•˜ì—¬ ì¤‘ë¦½ì„ í¬í•¨í•œ 7ê°€ì§€ ê°ì •ì„ ë¶„ë¥˜í•˜ëŠ” ë”¥ëŸ¬ë‹ ê¸°ë°˜ ê°ì • ì¸ì‹ ëª¨ë¸ì„ ê°œë°œí•˜ì˜€ìŠµë‹ˆë‹¤.
Multi-modal ë°ì´í„°ì˜ Alignmentë¥¼ ìœ„í•´ ë°°ì¹˜ ë‚´ ë‹¤ë¥¸ ìƒ˜í”Œë“¤ ì‚¬ì´ì˜ ê°™ì€ ëª¨ë‹¬ë¦¬í‹° ê°„ ê´€ê³„ë¥¼ ê³ ë ¤í•˜ëŠ” Intra-modal Contrastive Lossì™€ ë‹¤ë¥¸ ëª¨ë‹¬ë¦¬í‹° ê°„ ê´€ê³„ë¥¼ íŒŒì•…í•˜ê¸° ìœ„í•œ Inter-modal Contrastive Lossë¥¼ ì ìš©í•œ ë°©ë²•ì„ ì œì•ˆí•˜ì—¬ ê³µëª¨ì „ì—ì„œ ê°€ì¥ ë†’ì€ macro F1-scoreë¥¼ ë‹¬ì„±í•˜ì˜€ìŠµë‹ˆë‹¤.
6. **ë³´ë„ ìë£Œ:** [**[news1]](https://www.etnews.com/20230621000124)Â [[news2]](http://biz.heraldcorp.com/view.php?ud=20230621000315)Â [[news3]](https://www.gttkorea.com/news/articleView.html?idxno=5685)**

---

The "Han Hye-sung" team, composed of students from the Graduate School of Software Convergence at Kyung Hee University, won the Minister of Science and ICT Award at the **'2nd ETRI Human Understanding AI Paper Competition'** held by the Electronics and Telecommunications Research Institute (ETRI). 

(link : https://www.gttkorea.com/news/articleView.html?idxno=5685)

paper link : https://www.dbpia.co.kr/pdf/pdfView.do?nodeId=NODE11488658&googleIPSandBox=false&mark=0&ipRange=false&b2cLoginYN=false&isPDFSizeAllowed=true&accessgl=Y&language=ko_KR&hasTopBanner=true

This project implements a multimodal emotion recognition system based on the KEMDy20 dataset using text (BERT-based), audio (Wav2Vec2-based), and biosignal data. The project is organized in a modular fashion with separate components for data preprocessing, dataset creation, data loading, model definition, training, and evaluation.

---

## Repository Structure

```
LMR-CL/
â”œâ”€â”€ Data/                         # Raw and processed input data (KEMDy20_v1_1)
â”œâ”€â”€ checkpoints/                  # Saved model checkpoints
â”œâ”€â”€ config/                       # Configuration and hyperparameter definitions
â”‚   â””â”€â”€ config.py                 # CLI arguments and default settings
â”œâ”€â”€ data/                         # Preprocessed datasets and metadata
â”œâ”€â”€ dataloaders/                  # PyTorch Dataset & DataLoader implementations
â”œâ”€â”€ models/                       # Model architectures (LMR, etc.)
â”œâ”€â”€ previous_code_files/          # Legacy or reference code
â”œâ”€â”€ solvers/                      # Training and evaluation logic
â”‚   â”œâ”€â”€ solver_bert.py            # Text modality solver
â”‚   â”œâ”€â”€ solver_wav.py             # Audio modality solver
â”‚   â””â”€â”€ solver_wav_finetuning.py  # Audio fine-tuning solver
â”œâ”€â”€ train/                        # Entry-point training scripts
â”‚   â”œâ”€â”€ train_bert.py             # Train text subnetwork
â”‚   â”œâ”€â”€ train_wav.py              # Train audio subnetwork
â”‚   â””â”€â”€ train_wav_finetuning.py   # Fine-tune audio network
â”œâ”€â”€ utils/                        # Utility functions and helpers
â”œâ”€â”€ environment.yml               # Conda environment specification
â”œâ”€â”€ LICENSE                       # MIT License
â””â”€â”€ README.md                     # This file
```

## Prerequisites

- **Conda** (Miniconda or Anaconda)
- **Python** 3.7+
- **CUDA** 10.1 (for GPU-enabled training)

## Setup

1. Clone the repository:
   ```bash
   git clone https://github.com/hannn0403/LMR-CL.git
   cd LMR-CL
   ```
2. Create the Conda environment:
   ```bash
   conda env create -f environment.yml
   conda activate lmr_cl
   ```
3. Install any additional dependencies (if needed):
   ```bash
   pip install -r requirements.txt  # if you add one
   ```

## Data Preparation

1. Download the **KEMDy20_v1_1** dataset and extract it into the `Data/` directory:
   ```bash
   Data/
   â””â”€â”€ KEMDy20_v1_1/
       â”œâ”€â”€ audio/
       â”œâ”€â”€ text/
       â””â”€â”€ biosignal/
   ```
2. The `config/config.py` script will look for this path by default; adjust `--data_dir` if your directory differs.

## Configuration

All hyperparameters and experiment settings can be modified via command-line arguments in `config/config.py`. Key options include:

```bash
--mode [train|eval]         # Run mode
--data kemdy20             # Dataset name
--use_bert True            # Enable text modality
--use_cmd_sim True         # Use contrastive similarity loss
--batch_size 256
--n_epoch 3
--learning_rate 1e-4
--optimizer Adam
--activation relu
--diff_weight 0.1          # Contrastive loss weight
--recon_weight 0.3         # Reconstruction loss weight
```  

To view all options:
```bash
python -c "from config.config import get_config; get_config(parse=True)"
```

## Training

### Text-only Subnetwork

```bash
python train/train_bert.py \
  --mode train \
  --data kemdy20 \
  --use_bert True \
  --name bert_experiment
```

### Audio-only Subnetwork

```bash
python train/train_wav.py \
  --mode train \
  --data kemdy20 \
  --name wav_experiment
```

### Audio Fine-Tuning

```bash
python train/train_wav_finetuning.py \
  --mode train \
  --data kemdy20 \
  --name wav_ft_experiment
```

Checkpoints will be saved under `checkpoints/<experiment_name>/`.

## Evaluation

Use the corresponding solver to evaluate on the validation/test split:

```bash
python solvers/solver_bert.py \
  --mode eval \
  --load_checkpoint checkpoints/bert_experiment/best_model.pth
```

Replace `solver_bert.py` with `solver_wav.py` or `solver_wav_finetuning.py` as needed.

## Logging & Visualization

- Training logs are printed to console; integrate TensorBoard by adapting `SummaryWriter` calls in solvers.
- Use your preferred visualization tools to plot loss curves and modality alignment metrics.

## License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.

## References

- **Paper**: Han et al., *Learning Modality-Fused Representations with Contrastive Loss for Multimodal Emotion Recognition*.
- **Competition**: Minister of Science and ICT Award, 2nd ETRI Human Understanding AI Paper Competition.
- **Dataset**: [KEMDy20](https://github.com/kwonmheo/KEMDy20)  

---

